{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Potential use cases for ML Education"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !conda install -n python3 -y lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np \n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('https://raw.githubusercontent.com/jodb/sparkTestData/master/insurance_claims.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(data.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "le_count = 0\n",
    "\n",
    "# Iterate through the columns\n",
    "for col in data:\n",
    "    if data[col].dtype == 'object':\n",
    "        # If 2 or fewer unique categories\n",
    "        if len(list(data[col].unique())) <= 2:\n",
    "            # Train on the training data\n",
    "            le.fit(data[col])\n",
    "            # Transform both training and testing data\n",
    "            data[col] = le.transform(data[col])\n",
    "            \n",
    "            \n",
    "            # Keep track of how many columns were label encoded\n",
    "            le_count += 1\n",
    "            \n",
    "print('%d columns were label encoded.' % le_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style=\"white\")\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "f, ax = plt.subplots(figsize=(15, 15))\n",
    "\n",
    "# Generate a custom diverging colormap\n",
    "cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "\n",
    "sns.heatmap(data.corr(), cmap=cmap, vmax=.3, center=0, annot=False, square=True, linewidths=.5, cbar_kws={\"shrink\": .5}, ax=ax)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colum_name =[]\n",
    "unique_value=[]\n",
    "# Iterate through the columns\n",
    "for col in data:\n",
    "    if data[col].dtype == 'object':\n",
    "        # If 2 or fewer unique categories\n",
    "        colum_name.append(str(col)) \n",
    "        unique_value.append(data[col].nunique())\n",
    "table= pd.DataFrame()\n",
    "table['Col_name'] = colum_name\n",
    "table['Value']= unique_value\n",
    "            \n",
    "table=table.sort_values('Value',ascending=False)\n",
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# droping columns based on above result\n",
    "data.drop(['incident_location','policy_bind_date','incident_date','auto_model','insured_occupation','policy_number'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(20, 20))\n",
    "sns.countplot(x='insured_hobbies',hue='fraud_reported',data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['insured_hobbies']=data['insured_hobbies'].apply(lambda x :'Other' if x!='chess' and x!='cross-fit' else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(20, 20))\n",
    "sns.countplot(x='auto_make',hue='fraud_reported',data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['insured_hobbies'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.get_dummies(data)\n",
    "print('Training Features shape: ', data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(10, 10))\n",
    "sns.countplot(x='fraud_reported',data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr= data.corr()\n",
    "y=data['fraud_reported']\n",
    "X= data.drop('fraud_reported',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def lgb_f1_score(y_hat, data):\n",
    "    y_true = data.get_label()\n",
    "    y_hat = np.round(y_hat) # scikits f1 doesn't like probabilities\n",
    "    return 'f1', f1_score(y_true, y_hat), True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_lgb(X_train, X_test, y_train, y_test, test_df):\n",
    "    params = {\n",
    "        \"objective\" : \"binary\",\n",
    "        \"n_estimators\" : 1000,\n",
    "        \"reg_alpha\" : 0.5,\n",
    "        \"reg_lambda\" : 0.5,\n",
    "        \"n_jobs\" : -1,\n",
    "        \"colsample_bytree\" : .8,\n",
    "        \"min_child_weight\" : 8,\n",
    "        \"subsample\" : 0.8715623,\n",
    "        \"min_data_in_leaf\" : 30,\n",
    "        \"nthread\" : 4,\n",
    "        \"metric\" : \"f1\",\n",
    "        \"num_leaves\" : 10,\n",
    "        \"learning_rate\" : 0.01,\n",
    "        \"verbosity\" : -1,\n",
    "        \"seed\" :  60,\n",
    "        \"max_bin\" : 60,\n",
    "        \"max_depth\" : 3,\n",
    "        \"min_gain_to_split\" : .0222415,\n",
    "        \"scale_pos_weight\" : 1.4,\n",
    "        \"bagging_fraction\" : 0.8\n",
    "    }\n",
    "    \n",
    "    lgtrain = lgb.Dataset(X_train, label=y_train)\n",
    "    lgval = lgb.Dataset(X_test, label=y_test)\n",
    "    evals_result = {}\n",
    "    model = lgb.train(params, lgtrain, 10000, \n",
    "                      valid_sets=[lgtrain, lgval], \n",
    "                      early_stopping_rounds=100, \n",
    "                      verbose_eval=100, \n",
    "                      evals_result=evals_result,feval=lgb_f1_score)\n",
    "    \n",
    "    pred_test_y = model.predict(test_df, num_iteration=model.best_iteration)\n",
    "    return pred_test_y, model, evals_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test, model, evals_result = run_lgb(X_train, X_test, y_train, y_test, X_test)\n",
    "print(\"LightGBM Training Completed...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(y_test,pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "fpr, tpr, threshold = metrics.roc_curve(y_test, pred_test)\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "f, ax = plt.subplots(figsize=(10, 10))\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Plot feature importances...')\n",
    "ax = lgb.plot_importance(model, max_num_features=10)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
